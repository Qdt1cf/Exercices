{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie 2 : Prétraitement de textes avec NLTK\n",
    "\n",
    "Dans ce notebook, nous allons aborder les principales étapes de prétraitement de textes :\n",
    "- Chargement de données textuelles avec `pandas`\n",
    "- Nettoyage et normalisation (minuscules, suppression de la ponctuation, etc.)\n",
    "- Tokenisation\n",
    "- Suppression des stopwords\n",
    "- Lemmatisation ou racinisation (stemming)\n",
    "\n",
    "Nous allons utiliser la bibliothèque **NLTK** (Natural Language Toolkit) pour les opérations de traitement du langage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bibliothèques importées et ressources NLTK prêtes.\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# 1. Import des bibliothèques et téléchargement des ressources NLTK\n",
    "# ========================================================================\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "# Si nécessaire, décommentez les lignes suivantes pour télécharger les ressources requises\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "# Définition d'un jeu de stopwords (en anglais par défaut, peut s'adapter à d'autres langues)\n",
    "stop_words = set(stopwords.words('english'))  \n",
    "\n",
    "# Instanciation de la classe pour la lemmatisation et le stemming\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "print(\"Bibliothèques importées et ressources NLTK prêtes.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Création ou chargement d'un jeu de données textuelles\n",
    "\n",
    "Pour illustrer la démarche, nous créons un DataFrame minimal de textes factices.\n",
    "Dans un cas réel, on lirait un fichier CSV ou JSON via `pd.read_csv` ou `pd.read_json`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Hello world! This is a test message.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Cybersecurity is crucial in the modern world.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Visit http://example.com for more info!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>Buy now!!! Special OFFER, only 10$...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>Phishing attacks are on the rise. Stay safe.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                           text\n",
       "0   1           Hello world! This is a test message.\n",
       "1   2  Cybersecurity is crucial in the modern world.\n",
       "2   3      Visit http://example.com for more info!!!\n",
       "3   4          Buy now!!! Special OFFER, only 10$...\n",
       "4   5   Phishing attacks are on the rise. Stay safe."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# 2. Création d'un DataFrame d'exemple\n",
    "# ========================================================================\n",
    "data = {\n",
    "    \"id\": [1, 2, 3, 4, 5],\n",
    "    \"text\": [\n",
    "        \"Hello world! This is a test message.\",\n",
    "        \"Cybersecurity is crucial in the modern world.\",\n",
    "        \"Visit http://example.com for more info!!!\",\n",
    "        \"Buy now!!! Special OFFER, only 10$...\",\n",
    "        \"Phishing attacks are on the rise. Stay safe.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explication** :  \n",
    "- Nous avons 5 documents textuels, chacun ayant un champ `id` et un champ `text`.  \n",
    "- Les phrases contiennent divers éléments comme ponctuation, URL, symboles, etc.\n",
    "\n",
    "---\n",
    "# 3. Nettoyage et normalisation\n",
    "\n",
    "Les étapes de nettoyage et de normalisation incluent généralement :\n",
    "- Passer le texte en minuscules\n",
    "- Supprimer (ou remplacer) la ponctuation, les caractères spéciaux, les URLs, etc.\n",
    "- (Optionnel) Supprimer les nombres\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Hello world! This is a test message.</td>\n",
       "      <td>hello world this is a test message</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Cybersecurity is crucial in the modern world.</td>\n",
       "      <td>cybersecurity is crucial in the modern world</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Visit http://example.com for more info!!!</td>\n",
       "      <td>visit for more info</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>Buy now!!! Special OFFER, only 10$...</td>\n",
       "      <td>buy now special offer only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>Phishing attacks are on the rise. Stay safe.</td>\n",
       "      <td>phishing attacks are on the rise stay safe</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                           text  \\\n",
       "0   1           Hello world! This is a test message.   \n",
       "1   2  Cybersecurity is crucial in the modern world.   \n",
       "2   3      Visit http://example.com for more info!!!   \n",
       "3   4          Buy now!!! Special OFFER, only 10$...   \n",
       "4   5   Phishing attacks are on the rise. Stay safe.   \n",
       "\n",
       "                                   cleaned_text  \n",
       "0            hello world this is a test message  \n",
       "1  cybersecurity is crucial in the modern world  \n",
       "2                           visit for more info  \n",
       "3                    buy now special offer only  \n",
       "4    phishing attacks are on the rise stay safe  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# 3. Fonction de nettoyage de base\n",
    "# ========================================================================\n",
    "def clean_text(text):\n",
    "    # 1. Passage en minuscules\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 2. Suppression des URLs (exemple simple, peut être amélioré avec une regex plus précise)\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # 3. Suppression de la ponctuation\n",
    "    # string.punctuation contient : !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
    "    # On peut y ajouter des symboles spéciaux si besoin\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # 4. Suppression des chiffres (optionnel)\n",
    "    text = re.sub(r\"\\d+\", '', text)\n",
    "    \n",
    "    # On peut également supprimer les espaces multiples\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Application de la fonction à la colonne \"text\"\n",
    "df['cleaned_text'] = df['text'].apply(clean_text)\n",
    "\n",
    "df[['id', 'text', 'cleaned_text']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous observons maintenant la version \"nettoyée\" de chaque document.  \n",
    "Les URLs ont été supprimées, la ponctuation enlevée, les chiffres retirés, et tout est en minuscules.\n",
    "\n",
    "---\n",
    "# 4. Tokenisation\n",
    "\n",
    "La tokenisation consiste à découper le texte en unités (tokens), souvent des mots ou parfois des sous-mots.\n",
    "Ici, nous utilisons `word_tokenize` de NLTK.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>hello world this is a test message</td>\n",
       "      <td>[hello, world, this, is, a, test, message]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>cybersecurity is crucial in the modern world</td>\n",
       "      <td>[cybersecurity, is, crucial, in, the, modern, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>visit for more info</td>\n",
       "      <td>[visit, for, more, info]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>buy now special offer only</td>\n",
       "      <td>[buy, now, special, offer, only]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>phishing attacks are on the rise stay safe</td>\n",
       "      <td>[phishing, attacks, are, on, the, rise, stay, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                  cleaned_text  \\\n",
       "0   1            hello world this is a test message   \n",
       "1   2  cybersecurity is crucial in the modern world   \n",
       "2   3                           visit for more info   \n",
       "3   4                    buy now special offer only   \n",
       "4   5    phishing attacks are on the rise stay safe   \n",
       "\n",
       "                                              tokens  \n",
       "0         [hello, world, this, is, a, test, message]  \n",
       "1  [cybersecurity, is, crucial, in, the, modern, ...  \n",
       "2                           [visit, for, more, info]  \n",
       "3                   [buy, now, special, offer, only]  \n",
       "4  [phishing, attacks, are, on, the, rise, stay, ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# 4. Tokenisation\n",
    "# ========================================================================\n",
    "def tokenize_text(text):\n",
    "    tokens = word_tokenize(text)  # Tokenisation via NLTK\n",
    "    return tokens\n",
    "\n",
    "df['tokens'] = df['cleaned_text'].apply(tokenize_text)\n",
    "df[['id', 'cleaned_text', 'tokens']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On obtient ainsi un tableau de tokens pour chaque phrase nettoyée.\n",
    "\n",
    "---\n",
    "# 5. Suppression des stopwords\n",
    "\n",
    "Les *stopwords* sont des mots très fréquents qui n'apportent généralement pas de valeur sémantique forte \n",
    "(par exemple en anglais : \"the\", \"is\", \"in\", \"at\", etc.).  \n",
    "NLTK fournit une liste par langue, ici nous utilisons l'anglais.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tokens_no_stop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[hello, world, this, is, a, test, message]</td>\n",
       "      <td>[hello, world, test, message]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[cybersecurity, is, crucial, in, the, modern, ...</td>\n",
       "      <td>[cybersecurity, crucial, modern, world]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>[visit, for, more, info]</td>\n",
       "      <td>[visit, info]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>[buy, now, special, offer, only]</td>\n",
       "      <td>[buy, special, offer]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>[phishing, attacks, are, on, the, rise, stay, ...</td>\n",
       "      <td>[phishing, attacks, rise, stay, safe]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                             tokens  \\\n",
       "0   1         [hello, world, this, is, a, test, message]   \n",
       "1   2  [cybersecurity, is, crucial, in, the, modern, ...   \n",
       "2   3                           [visit, for, more, info]   \n",
       "3   4                   [buy, now, special, offer, only]   \n",
       "4   5  [phishing, attacks, are, on, the, rise, stay, ...   \n",
       "\n",
       "                            tokens_no_stop  \n",
       "0            [hello, world, test, message]  \n",
       "1  [cybersecurity, crucial, modern, world]  \n",
       "2                            [visit, info]  \n",
       "3                    [buy, special, offer]  \n",
       "4    [phishing, attacks, rise, stay, safe]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# 5. Suppression des stopwords\n",
    "# ========================================================================\n",
    "def remove_stopwords(token_list):\n",
    "    filtered_tokens = [token for token in token_list if token not in stop_words]\n",
    "    return filtered_tokens\n",
    "\n",
    "df['tokens_no_stop'] = df['tokens'].apply(remove_stopwords)\n",
    "df[['id', 'tokens', 'tokens_no_stop']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous observons que des mots comme \"is\", \"in\", \"the\", \"are\", etc. ont disparu.\n",
    "\n",
    "---\n",
    "# 6. Lemmatisation ou Stemming\n",
    "\n",
    "- **Stemming** : on réduit le mot à sa racine (par ex. `attacks` -> `attack`), parfois au risque d'être agressif (ex. `studies` -> `studi`).\n",
    "- **Lemmatisation** : on ramène le mot à sa forme canonique en tenant compte de la partie du discours (ex. `better` -> `good`, `attacks` -> `attack`).\n",
    "\n",
    "Ici, nous allons illustrer la lemmatisation avec `WordNetLemmatizer` et le stemming avec `PorterStemmer`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tokens_no_stop</th>\n",
       "      <th>tokens_lemmatized</th>\n",
       "      <th>tokens_stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[hello, world, test, message]</td>\n",
       "      <td>[hello, world, test, message]</td>\n",
       "      <td>[hello, world, test, messag]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[cybersecurity, crucial, modern, world]</td>\n",
       "      <td>[cybersecurity, crucial, modern, world]</td>\n",
       "      <td>[cybersecur, crucial, modern, world]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>[visit, info]</td>\n",
       "      <td>[visit, info]</td>\n",
       "      <td>[visit, info]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>[buy, special, offer]</td>\n",
       "      <td>[buy, special, offer]</td>\n",
       "      <td>[buy, special, offer]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>[phishing, attacks, rise, stay, safe]</td>\n",
       "      <td>[phishing, attack, rise, stay, safe]</td>\n",
       "      <td>[phish, attack, rise, stay, safe]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                           tokens_no_stop  \\\n",
       "0   1            [hello, world, test, message]   \n",
       "1   2  [cybersecurity, crucial, modern, world]   \n",
       "2   3                            [visit, info]   \n",
       "3   4                    [buy, special, offer]   \n",
       "4   5    [phishing, attacks, rise, stay, safe]   \n",
       "\n",
       "                         tokens_lemmatized  \\\n",
       "0            [hello, world, test, message]   \n",
       "1  [cybersecurity, crucial, modern, world]   \n",
       "2                            [visit, info]   \n",
       "3                    [buy, special, offer]   \n",
       "4     [phishing, attack, rise, stay, safe]   \n",
       "\n",
       "                         tokens_stemmed  \n",
       "0          [hello, world, test, messag]  \n",
       "1  [cybersecur, crucial, modern, world]  \n",
       "2                         [visit, info]  \n",
       "3                 [buy, special, offer]  \n",
       "4     [phish, attack, rise, stay, safe]  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# 6. Lemmatisation et Stemming\n",
    "# ========================================================================\n",
    "\n",
    "def lemmatize_tokens(token_list):\n",
    "    # Par défaut, WordNetLemmatizer lemmatise en supposant une partie du discours \"n\" (nom)\n",
    "    # Pour un meilleur résultat, on peut faire un POS-tagging, puis lemmatiser en fonction du tag.\n",
    "    lemmatized = [lemmatizer.lemmatize(token) for token in token_list]\n",
    "    return lemmatized\n",
    "\n",
    "def stem_tokens(token_list):\n",
    "    stemmed = [stemmer.stem(token) for token in token_list]\n",
    "    return stemmed\n",
    "\n",
    "df['tokens_lemmatized'] = df['tokens_no_stop'].apply(lemmatize_tokens)\n",
    "df['tokens_stemmed'] = df['tokens_no_stop'].apply(stem_tokens)\n",
    "\n",
    "df[['id', 'tokens_no_stop', 'tokens_lemmatized', 'tokens_stemmed']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez comparer la différence entre la forme \"lemmatisée\" et la forme \"racinisée\" (stemmée) pour chaque token.\n",
    "\n",
    "---\n",
    "# 7. Conclusion\n",
    "\n",
    "Nous avons illustré les principales étapes de prétraitement de textes avec NLTK :\n",
    "\n",
    "1. Passage en minuscules et nettoyage (suppression de URLs, ponctuation, chiffres, espaces multiples).\n",
    "2. Tokenisation avec `word_tokenize`.\n",
    "3. Suppression des *stopwords*.\n",
    "4. Lemmatisation ou racinisation.\n",
    "\n",
    "Ces opérations constituent les bases du pipeline NLP avant la phase de vectorisation (TF-IDF, embeddings, etc.) \n",
    "et de modélisation (par exemple, classification).\n",
    "\n",
    "Dans un projet réel, on adaptera chacune de ces étapes :\n",
    "- Regex plus complexes pour traiter certains patterns,\n",
    "- Liste de stopwords personnalisée,\n",
    "- Lemmatisation multilingue,\n",
    "- etc.\n",
    "\n",
    "Ensuite, vous pourrez intégrer ce prétraitement dans votre pipeline de classification ou d'analyse de texte.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
