{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, IFrame\n",
    "from IPython.core.display import HTML\n",
    "from IPython.display import Latex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron and theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A linear perceptron maps an input $x \\in \\mathbb{R}^n$ ($n$ values) to an output which can be a binary output $F(x) \\in \\{0,1\\}$ (1 value which is one or zero).\n",
    "\n",
    "This function is decomposed in two parts : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **linear function** : defined by $n$ weights $a_1,...,a_n$ :\n",
    "$$f(x) = a_1x_1 + a_2x_2 +...+ a_n x_n$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example : $n=2$, $(a_1,a_2) = (2,3)$ so that $f(x_1,x_2) = 2x_1 + 3x_2$ and $f(4,-1) = 5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([4,-1])  #n-vector x\n",
    "a = np.array([2,3])   #n weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x,a):\n",
    "    return sum(x*a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f(x,a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **activation function** : it is a function $\\varphi \\colon \\mathbb{R} \\to \\mathbb{R}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example : Heaviside function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def H(y):\n",
    "    if y<0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H(-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H(f(x,a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=np.linspace(-6,6,300)\n",
    "z=np.array([H(t) for t in y])\n",
    "plt.plot(y,z,linewidth=7.0,color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is the number $H(f(x)) \\in \\{0,1\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example : ReLu function\n",
    "\n",
    "The Rectified Linear Unit function is the following :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLu(y):\n",
    "    if y<0:\n",
    "        return 0\n",
    "    else:\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=np.linspace(-6,6,300)\n",
    "z=np.array([ReLu(t) for t in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y,z,linewidth=7.0,color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example : Sigmoïd function\n",
    "\n",
    "The Sigmoïd function is the following :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(y):\n",
    "    return 1/(1+np.exp(-y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=np.linspace(-6,6,300)\n",
    "z=np.array([sigmoid(t) for t in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y,z,linewidth=7.0,color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise : compute the derivative of the sigmoïd function and plot its graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dsigmoid(y):\n",
    "    return np.exp(-y)/(1+np.exp(-y))**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=np.linspace(-6,6,300)\n",
    "z=np.array([sigmoid(t) for t in y])\n",
    "dz=np.array([dsigmoid(t) for t in y])\n",
    "#plt.plot(y,z,linewidth=7.0,color=\"red\")\n",
    "plt.plot(y,dz,linewidth=4.0,color=\"blue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neuron representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A linear pereceptron is represented with a **neuron** as follows : <img src=\"img/Perceptron1.png\"> </img> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$n=2$, $(a_1,a_2) = (2,3)$ so that $f(x_1,x_2) = 2x_1 + 3x_2$ and the activation function Heaviside : the output is\n",
    "\n",
    "- 1 if $2x_1+3x_2 \\geq 0$ ;\n",
    "- 0 otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(IFrame('https://www.geogebra.org/calculator/x63nersc?embed',900,400))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "How to find a perceptron separating blue circles from red squares ?\n",
    "https://www.geogebra.org/calculator/zmyaznwf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(IFrame('https://www.geogebra.org/calculator/zmyaznwf?embed',600,400))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affine perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We introduce a biais $a_0$ : this a $(n+1)$-th weight which defines an affine function $f(x_1,...,x_n) = a_1x_1+...+a_nx_n + a_0$. An affine pereceptron is represented by a neuron as follows :\n",
    "<img src=\"Perceptron2.png\"> </img> \n",
    "\n",
    "Example : with 2 entries, an affine perceptron split a 2-dimensional space into 2 half planes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Or, and, xor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In computer science, a boolean variable is a variable $x$ that has 1 of 2 possible values (TRUE or FALSE). In a boolean algebra, if $x$ et $y$ are boolean, we can define ```x OR y```.\n",
    "\n",
    "We chose a graphic representation : TRUE is number 1, FALSE is number 0, ```x OR y``` is a point with $(x,y)$ coordinates in plane. This point is a red square if ```x OR y = TRUE```, a blue circle otherwise.\n",
    "\n",
    "<img src=\"img/or_perceptron1.png\"> </img> \n",
    "\n",
    "1. Can I realize this operation ```x OR y``` with a perceptron ?\n",
    "\n",
    "<img src=\"img/or_perceptron2.png\"> </img> \n",
    "\n",
    "Yes ! For example, take the weights $(a_1,a_2,a_0) = (1,1,-1)$. \n",
    "\n",
    "2. In the same way, can I realize the operation ```x AND y``` with a perceptron ?\n",
    "\n",
    "<img src=\"img/and_perceptron1.png\"> </img> \n",
    "\n",
    "Yes ! For example, take the weights $(a_1,a_2,a_0) = (1,1,-1.5)$. \n",
    "\n",
    "3. In the same way, can I realize the operation ```x XOR y``` with a perceptron ?\n",
    "\n",
    "<img src=\"img/xor_perceptron.png\"> </img> \n",
    "\n",
    "No ! You cannot find a straight line that separates these two kind of points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is to know that the two sets of points are **linearly separable**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "In an $n$-dimensionnal Euclidian space, two sets of points $A$ and $B$ are linearly separable if an hyperplane can separate space : there exists $a_1,...,a_n,a_0$ such that for each $x \\in A$, $\\sum_{i=1}^n a_i x_i +a_0> 0$ and for each $x \\in B$, $\\sum_{i=1}^n a_i x_i + a_0 < 0$.     \n",
    "    An another way to say that is their respective convex hulls are disjoint.\n",
    " </div>\n",
    " \n",
    " <div class=\"alert alert-danger\" role=\"alert\">\t\n",
    "\n",
    "In an $n$-dimensionnal Euclidian space, two sets of points $A$ and $B$ are linearly separable if there exists a perceptron that takes value 1 on $A$ and 0 on $B$.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A perceptron is a **linear classifier**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise \n",
    "\n",
    "Is it possible to realize the operation ```x OR y OR z``` with a perceptron ?\n",
    "\n",
    "<img src=\"img/or_or_perceptron.png\"> </img> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wow, your perceptron is learning for the first time !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This learning rule is an example of supervised training, in which the learning rule is provided\n",
    "with a set of examples of proper perecptron behavior: a collection of $(x,t)$ where $x$ is an input and $t$ is a the corresponding target output. As each input is applied to the network, the network output is compared\n",
    "to the target. **The learning rule then adjusts the weights and biases\n",
    "of the perceptron in order to move the perceptron output closer to the target.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test problem\n",
    "These are 3  input/target pairs for our test problem :\n",
    "$$x_1 = (1,2) \\,;\\, t_1 = 1 \\qquad x_2 = (-1,2) \\,;\\, t_2 = 0 \\qquad x_3 = (0,-1) \\,;\\, t_3 = 0$$\n",
    "\n",
    "The perceptron for this problem should have two-inputs and one output. To\n",
    "simplify our development of the learning rule, we will begin with a network\n",
    "without a bias so that we are looking for two weights $a_1,a_2$. Activation function is Heaviside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[];t=[]\n",
    "x.append(np.array([1,2])) ; t.append(1)\n",
    "x.append(np.array([-1,2])) ; t.append(0)\n",
    "x.append(np.array([0,-1])) ; t.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constructing Learning Rules\n",
    "We set the weight vector $w = (a_1,a_2)$ to the following randomly generated values: $w = (1.0, -0.8)$. \n",
    "\n",
    "Then we execute the perceptron with the first input $x_1$ : output $y_1$ is equal to $0 \\neq t_1$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([1.0,-0.8])\n",
    "y = sum(w*x[0])\n",
    "print(y)\n",
    "y = H(y)\n",
    "print(y)\n",
    "y == t[0] #test if output y_1 is equal to t_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rule** : if $t=1$ and $y=0$ then $w_{new} := w_{old} + x$.\n",
    "\n",
    "Then we execute the perceptron with the second input $x_2$ and new weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = w + x[0]\n",
    "y = H(sum(w*x[1]))\n",
    "y == t[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rule** : if $t=0$ and $y=1$ then $w_{new} := w_{old} - x$.\n",
    "\n",
    "Then we execute the perceptron with the second input $x_3$ and new weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = w - x[1]\n",
    "y = H(sum(w*x[2]))\n",
    "y == t[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y - t[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the previous rule :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = w - x[2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we check :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    y = H(sum(w*x[i]))\n",
    "    print(y == t[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unfying Learning Rules\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This rule can be extended to train the bias by noting that a bias is simply\n",
    "a weight whose input is always 1.\n",
    "\n",
    "#### Exercise :\n",
    "Write a program that applies all the rules given above with different weights at start. Check the result.\n",
    "\n",
    "Bonus : Represent with a 2d-graph the evolution of $w$ by plotting the corresponding linear classifyer.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n = len(x)\n",
    "w = np.array([-0.5,-0.9])\n",
    "for i in range(n):\n",
    "    y = H(sum(w*x[i]))\n",
    "    err = t[i]-y\n",
    "    w = w + err * x[i]\n",
    "for i in range(3):\n",
    "    y = H(sum(w*x[i]))\n",
    "    print(y == t[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, given some input $(x_1,...,x_n)$, we organise multiple neurons in **layers**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.researchgate.net/profile/Facundo_Bre/publication/321259051/figure/fig1/AS:614329250496529@1523478915726/Artificial-neural-network-architecture-ANN-i-h-1-h-2-h-n-o.png\"> </img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 0 : a neural network with 2 layers : 2 neurons (linear perceptron with ReLu activation function) on the 1st layer, 1 neuron (affine perceptron with Heavyside activation function) on the 2nd layer.\n",
    "\n",
    "<img src=\"img/neural_layer_ex1.png\"> </img>\n",
    "\n",
    "**Exercise** : check that output is $1$ if input is $(4,7)$.\n",
    "\n",
    "_Tip_ : we can use a matricial product : in Python, $A \\times B$ is computed with this command : ```np.dot(A,B)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLu(y):\n",
    "    if y<0:\n",
    "        return 0\n",
    "    else:\n",
    "        return y\n",
    "\n",
    "def H(y):\n",
    "    if y<0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([4,7])\n",
    "W1 = np.array([[2,-1],[-3,2]])\n",
    "Y1 = np.dot(W1,X)\n",
    "Y1 = [ReLu(y) for y in Y1]\n",
    "W2 = np.array([4,5])\n",
    "b2 = -1\n",
    "Y2 = np.dot(W2,Y1)+b2\n",
    "Y2 = H(Y2)\n",
    "print(Y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1\n",
    "<img src=\"img/neural_layer_ex2.png\"> </img>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1st layer, 1st neuron :\n",
    "Is active if $-x+3y \\geq 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.linspace(-6,6,300)\n",
    "y=np.array([t/3 for t in x])\n",
    "plt.plot(x,y,'k--')\n",
    "plt.fill_between(x,y,np.max(y)+2,color=\"green\",alpha=0.5)\n",
    "\n",
    "plt.axis('equal')\n",
    "plt.axis([-5,5,-3,3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1st layer, 2nd neuron :\n",
    "Is active if $2x+y \\geq 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.linspace(-6,6,300)\n",
    "z=np.array([-2*t for t in x])\n",
    "plt.plot(x,z,'k--')\n",
    "plt.fill_between(x,z,np.max(z),color=\"green\",alpha=0.5)\n",
    "\n",
    "plt.axis('equal')\n",
    "plt.axis([-5,5,-3,3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2nd layer, one neuron :\n",
    "\n",
    "The output neuron realize the boolean function ```x AND y``` (c.f. previous course)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### result of two layers :\n",
    "\n",
    "The neural network has output equal to $1$ on the __intersection__ of the two half planes where neurones of 1st layer are equal to $1$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x=np.linspace(-6,6,300)\n",
    "y=np.array([t/3 for t in x])\n",
    "z=np.array([-2*t for t in x])\n",
    "plt.plot(x,y,'k--')\n",
    "plt.plot(x,z,'k--')\n",
    "plt.fill_between(x,y,np.max(y)+2,color=\"green\",alpha=0.3)\n",
    "plt.fill_between(x,z,np.max(z),color=\"green\",alpha=0.3)\n",
    "plt.fill_between(x,z,np.max(z), where=y<z,color=\"red\",alpha=1)\n",
    "plt.fill_between(x,y,np.max(z), where=y>z,color=\"red\",alpha=1)\n",
    "plt.axis('equal')\n",
    "plt.axis([-5,5,-3,3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2\n",
    "<img src=\"img/neural_layer_ex3.png\"> </img>\n",
    "\n",
    "Comments : 1st layer is the same than before, output neuron is the ```or```neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.linspace(-6,6,300)\n",
    "y=np.array([t/3 for t in x])\n",
    "z=np.array([-2*t for t in x])\n",
    "plt.plot(x,y,'k--')\n",
    "plt.plot(x,z,'k--')\n",
    "plt.fill_between(x,y,np.max(y)+2,color=\"red\",alpha=1)\n",
    "plt.fill_between(x,z,np.max(z),color=\"red\",alpha=1)\n",
    "#plt.fill_between(x,z,np.max(z), where=y<z,color=\"red\",alpha=0.5)\n",
    "#plt.fill_between(x,y,np.max(z), where=y>z,color=\"red\",alpha=0.5)\n",
    "plt.axis('equal')\n",
    "plt.axis([-5,5,-3,3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network has output equal to $1$ on the __union__ of the two half planes where neurones of 1st layer are equal to $1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise :\n",
    "Find a neural network whose output is 1 on the red area, 0 otherwise.\n",
    "<img src=\"img/neural_layer_ex4.png\"> </img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def H(y):\n",
    "    if y<0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def nnetwork(x,y):\n",
    "    X = np.array([x,y])\n",
    "    W1 = np.array([[-1,-3],[-3,5],[-5,-1]])\n",
    "    B1 = np.array([7,7,-7])\n",
    "    W2 = np.array([[1,1,1]])\n",
    "    B2 = -3\n",
    "    layer1 = np.dot(W1,X)+B1\n",
    "    layer1 = np.array([H(t) for t in layer1])\n",
    "    layer2 = np.dot(W2,layer1)+B2\n",
    "    layer2 = H(layer2)\n",
    "    output = layer2\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=100\n",
    "xmin = -6\n",
    "xmax = 5\n",
    "ymin = -5\n",
    "ymax = 5\n",
    "X = np.linspace(xmin,xmax,n)\n",
    "Y = np.linspace(ymin,ymax,n)\n",
    "for x in X:\n",
    "    for y in Y:\n",
    "        if nnetwork(x,y) == 1:\n",
    "            plt.plot(x,y,'.r')\n",
    "plt.axis('equal')\n",
    "plt.axis([xmin,xmax,ymin,ymax])\n",
    "xtick=np.linspace(xmin,xmax,xmax-xmin+1)\n",
    "ytick=np.linspace(ymin,ymax,ymax-ymin+1)\n",
    "plt.xticks(xtick)\n",
    "plt.yticks(ytick)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to realize ```XOR``` ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise : \n",
    "We have seen that one neuron is not sufficient to realize the operation ```XOR```. Find a neural network with two layers that realize the operation ```XOR```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer : \n",
    "<img src=\"img/xor_neuralnetwork.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realizable sets : definition and properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "A set $A$ is NN-realizable if it exists a neural network whose output is equal to $1$ in $A$, $0$ outside of $A$.\n",
    "   </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\" role=\"alert\">\n",
    "Every convex $n$-polygon in $\\mathbb{R}^2$ is NN-realizable with $n+1$ neurons. \n",
    "   </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\" role=\"alert\">\n",
    "If $A$ et $B$ are two NN-realizable sets then :\n",
    "   <ol> \n",
    "       <li> $A \\cup B$ is NN-realizable ;</li>\n",
    "    <li> $A \\cap B$ is NN-realizable ;</li>  \n",
    "    <li> $\\overline{A}$ is NN-realizable ;</li>\n",
    "    <li> $A \\backslash B$ is NN-realizable.</li>\n",
    "    </ol>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\" role=\"alert\">\n",
    "Every polygon in $\\mathbb{R}^2$ is NN-realizable. Then, every Jordan curve (simpe closed curve) can be approximated by a neural network. \n",
    "   </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Universal approximation theorem \n",
    "Goal : approximate every continuous function $\\mathbb{R} \\to \\mathbb{R}$ by a neural network. More precisely, let $f \\colon [a;b] \\to \\mathbb{R}$ : we want to find a neural network whose output $F(x) \\approx f(x)$ for all $x \\in [a;b]$. To do this, assume that the output neuron has the identity $x \\mapsto x$ activation function. Other neurons have Heaviside activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heaviside step functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First trivial case :\n",
    "<img src=\"img/step_function1.png\"></img>\n",
    "\n",
    "Its is easy to shift the step on the left :\n",
    "<img src=\"img/step_function2.png\"></img>\n",
    "\n",
    "or and the right :\n",
    "<img src=\"img/step_function3.png\"></img>\n",
    "\n",
    "step down :\n",
    "<img src=\"img/step_function4.png\"></img>\n",
    "<img src=\"img/step_function5.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rectangular functions\n",
    "Just add two Heaviside step functions !\n",
    "<img src=\"img/rect_function.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step functions\n",
    "Just add some rectangular functions ! Be careful if the rectangles ar contiguous.\n",
    "<img src=\"img/step_function6.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continuous functions\n",
    "Finally, notice that every continuous function $[a;b] \\to \\mathbb{R}$ can be uniformly approximated by a step function.\n",
    "\n",
    "<img src=\"img/approx_function.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In higher dimension :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise :\n",
    "Find a neural network that performs this 2-dimensionnal function :\n",
    "\n",
    "<img src=\"img/step_function7.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer : \n",
    "\n",
    "<img src=\"img/neural_2dim_step.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training rule for a 1-layer network\n",
    "\n",
    "Let $W$ be the weight matrix : each row is corresponding to a neuron of the layer. Let $b$ be the column matrix of bias : each row is corresponding to a neuron of the layer. \n",
    "\n",
    "Let $e$ be the column matrix of error, then the training rule is:\n",
    "\n",
    "$$W^{new} = W^{old} + e \\times x^T$$\n",
    "$$b^{new} = b^{old} + e$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. How to pick an architecture\n",
    "\n",
    "Problem specifications help define the network in the following ways:\n",
    "1. Number of network inputs = number of problem inputs\n",
    "2. Number of neurons in output layer = number of problem outputs\n",
    "3. Output layer transfer function choice at least partly determined by\n",
    "problem specification of the outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "A single-layer neural network is to have six inputs and two outputs.\n",
    "The outputs are to be limited to and continuous over the\n",
    "range 0 to 1. What can you tell about the network architecture?\n",
    "Specifically:\n",
    "* How many neurons are required?\n",
    "* What are the dimensions of the weight matrix?\n",
    "* What kind of transfer functions could be used?\n",
    "* Is a bias required?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: \n",
    "* Two neurons, one for each output, are required.\n",
    "* The weight matrix has two rows corresponding to the two neurons and\n",
    "six columns corresponding to the six inputs. (The product is a two-element\n",
    "vector.)\n",
    "* Of the transfer functions we have discussed, the transfer function\n",
    "would be most appropriate.\n",
    "* Not enough information is given to determine if a bias is required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem\n",
    "\n",
    "We have a classification problem with four classes of input vector. The four classes are : \n",
    "* class 1 : $x_1 = (1,1)$ and $x_2 = (1,2)$\n",
    "* class 2 : $x_3 = (2,-1)$ and $x_4 = (2,0)$\n",
    "* class 3 : $x_5 = (-1,2)$ and $x_6 = (-2,1)$\n",
    "* class 4 : $x_7 = (-1,-1)$ and $x_8 = (-2,-2)$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Design a neural network to solve this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need 2 neurons and check if we can divide the 4 classes into 2 sets of 2.\n",
    "\n",
    "<img src = \"img/NNproblem1.png\"> </img>\n",
    "\n",
    "The answer is yes. \n",
    "\n",
    "Then we have to choose which value is expected according to the class of input. Let us choose theses target values :\n",
    "\n",
    "* class 1 : $t_1 = t_2 = (0,0)$\n",
    "* class 2 : $t_3 = t_4 = (0,1)$\n",
    "* class 3 : $t_5 = t_6 = (1,0)$\n",
    "* class 4 : $t_7 = t_8 = (1,1)$\n",
    "\n",
    "Then we can  graphically find suitable weights for each neuron: $w_1 = (-3,-1)$ and $w_2 = (1,-2)$. \n",
    "\n",
    "It is easy to find correct bias by picking a point on each boundary line: $b_1 = 1$ and $b_2 = 0$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Train a perceptron network to solve this problem\n",
    "using the perceptron learning rule.\n",
    "\n",
    "_Tip: be careful of the size of your matrix when you make a product. Here is an example of product:_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient for a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider a neural network with $n$ input $(x_1,...,x_n)$ and one output. That defines a fonction $F \\colon \\mathbb{R}^n \\to \\mathbb{R}$, $(x_1,...,x_n) \\mapsto F(x_1,...,x_n)$.\n",
    "\n",
    "Let $(a_1,...,a_m)$ be the set of weights of this neural network. Now we consider $(a_1,...,a_m)$ as variables and the function $\\widetilde{F} \\colon (a_1,...,a_m) \\mapsto \\widetilde{F}(a_1,...,a_m)$. \n",
    "\n",
    "In particular, we are interested in \n",
    "$$\\frac{\\partial \\widetilde{F}}{\\partial a_j}$$ for all $j \\in \\{1,...,m\\}$ and then the gradient of a cost function $E = (\\widetilde{F}-y_0)^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "\n",
    "#### Algorithm : \n",
    "It gives a sequence $x_0,x_1,...$ defined by this algorithm :\n",
    "1. Compute gradient : $\\nabla f(x_k)$ ;\n",
    "2. Stopping criteria : $||\\nabla f(x_k)||<\\varepsilon$ ;\n",
    "3. Choose a step value $\\alpha_k >0$ ;\n",
    "4. Iteration : $x_{k+1} = x_k - \\alpha_k \\nabla f(x_k)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from descent import *\n",
    "from descente_stochastique import *\n",
    "from descente_lot import *\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def exemple1():\n",
    "    # 2-var function\n",
    "    def f(x, y):\n",
    "        return x**2 + 3*y**2\n",
    "    \n",
    "    # handmade gradient\n",
    "    def grad_f(x, y):\n",
    "        g = [2*x, 6*y]\n",
    "        return np.array(g)\n",
    "\n",
    "    # Test\n",
    "    print(\"--- gradient descent ---\")\n",
    "    X0 = np.array([2, 1])    \n",
    "    my_step = 0.2\n",
    "    X0 = np.array([-1, -1])    \n",
    "    my_step = 0.1    \n",
    "    display_descent(f, grad_f, X0, delta=my_step, nmax = 21)\n",
    "    graphic_descent_2var_2d(f, grad_f, X0, delta=my_step, nmax = 10, zone = (-2.5,2.5,-1.5,1.5) ) \n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exemple1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First example with one neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "We want to separate the plane according to these two sets of points : \n",
    "\n",
    "blue circles : (0, 3), (1, 1.5), (1, 4), (1.5, 2.5), (2, 2.5), (3, 3.5), (3.5, 3.25), (4, 3), (4, 4), (5, 4)\n",
    "\n",
    "red squares : (1, 1), (2, 0.5), (2, 2), (3, 1.5), (3, 2.75), (4, 1), (4, 2.5), (4.5, 3), (5, 1), (5, 2.25).\n",
    "\n",
    "with a single neuron perceptron : \n",
    "\n",
    "<img src=\"img/propagation_ex1.png\"></img>\n",
    "\n",
    "The activation function is the sigmoid function.\n",
    "\n",
    "<img src=\"img/propagation_ex1_1.png\"></img>\n",
    "\n",
    "The cost function is \n",
    "$$E(a,b,c) = \\frac{1}{N}\\sum_{i=1}^N E_i(a,b,c)$$\n",
    "where $E_i = (F(x_i,y_i)-t_i)^2$ and $t_i=1$ when $(x_i,y_i)$ is a red square, $t_i=0$ when $(x_i,y_i)$ is a blue circle. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data : training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blue_points = [(0, 3), (1, 1.5), (1, 4), (1.5, 2.5), (2, 2.5), (3, 3.5), (3.5, 3.25), (4, 3), (4, 4), (5, 4)]\n",
    "red_points = [(1, 1), (2, 0.5), (2, 2), (3, 1.5), (3, 2.75), (4, 1), (4, 2.5), (4.5, 3), (5, 1), (5, 2.25)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = []\n",
    "points = []\n",
    "for x,y in blue_points:\n",
    "    target.append(0)\n",
    "    points.append((x,y))\n",
    "    plt.scatter(x,y,color='blue')\n",
    "for x,y in red_points:\n",
    "    target.append(1)\n",
    "    points.append((x,y))\n",
    "    plt.scatter(x,y,color='red',marker='s')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Gradient descent\n",
    "\n",
    "We want to find the best weights $W=(a,b,c)$ by iteration : initialize $W_0 = (a_0,b_0,c_0)$, for example $W_0 = (0,1,-2)$ and fix a step value $\\delta = 1$. The sequence of weights is define by\n",
    "$$W_{k+1} = W_k - \\delta \\cdot \\nabla E(W_k)$$\n",
    "\n",
    "The local error is :\n",
    "$$E_i(a,b,c) = (\\sigma(ax_i+by_i+c)-t_i)^2$$\n",
    "\n",
    "and the error is the sum of local error where $N$ is the number of points in the training set :\n",
    "\n",
    "$$E(a,b,c) = \\frac{1}{N}\\sum_{i=1}^N E_i(a,b,c)$$\n",
    "\n",
    "Notice that $\\sigma' = \\sigma(1-\\sigma)$ so that \n",
    "$$\\frac{\\partial E_i}{\\partial a}(x_i,y_i) = 2x_i \\sigma_i(1-\\sigma_i)(\\sigma_i-t_i)$$\n",
    "$$\\frac{\\partial E_i}{\\partial b}(x_i,y_i) = 2y_i \\sigma_i(1-\\sigma_i)(\\sigma_i-t_i)$$\n",
    "$$\\frac{\\partial E_i}{\\partial c}(x_i,y_i) = 2 \\sigma_i(1-\\sigma_i)(\\sigma_i-t_i)$$\n",
    "where $\\sigma_i = \\sigma(ax_i+by_i+c)$.\n",
    "\n",
    "Finaly, $$\\nabla E(W_k) = \\frac{1}{N}\\sum_{i=1}^N \\left[\\frac{\\partial E_i}{\\partial a},\\frac{\\partial E_i}{\\partial b},\\frac{\\partial E_i}{\\partial c}\\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(y):\n",
    "    return 1/(1+np.exp(-y))\n",
    "\n",
    "def p(y):\n",
    "    return y*(1-y)\n",
    "\n",
    "def dsigmoid(y):\n",
    "    return p(sigmoid(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.array([0,1,-2])\n",
    "\n",
    "\n",
    "def gradE(W,x,y,t):\n",
    "    sigma = sigmoid(np.dot(W[:2],np.array([x,y]))+W[2])\n",
    "    gradEa = 2*x*sigma*(1-sigma)*(sigma-t)\n",
    "    gradEb = 2*y*sigma*(1-sigma)*(sigma-t)\n",
    "    gradEc = 2*sigma*(1-sigma)*(sigma-t)\n",
    "    return gradEa,gradEb,gradEc\n",
    "\n",
    "def E(x,y,W,t):\n",
    "    return (sigmoid(np.dot(W[:2],np.array([x,y]))+W[2])-t)**2\n",
    "\n",
    "def gradE_total(W):\n",
    "    g = np.array([0,0,0])\n",
    "    i=0\n",
    "    for (x,y) in points:\n",
    "        g = np.array(gradE(W,x,y,target[i])) + g\n",
    "        i+=1\n",
    "    g=g/(i)\n",
    "    return g\n",
    "\n",
    "def E_total(W):\n",
    "    e = 0.0\n",
    "    i=0\n",
    "    for (x,y) in points:\n",
    "        e = E(x,y,W,target[i]) + e\n",
    "        i+=1\n",
    "    e = e/(i)\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init\n",
    "W = np.array([0,1,-2])\n",
    "epoch = 1000  #change number of iterations\n",
    "delta = 1\n",
    "error = []\n",
    "\n",
    "for i in range(epoch):\n",
    "    W = W - delta*gradE_total(W)\n",
    "    error.append(E_total(W))\n",
    "\n",
    "print('(a,b,c) = '+str(W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y in blue_points:\n",
    "    plt.scatter(x,y,color='blue')\n",
    "for x,y in red_points:\n",
    "    plt.scatter(x,y,color='red',marker='s')\n",
    "h = np.array([-0.1,5.1])\n",
    "v = (-W[0]*h-W[2])/W[1]  #equation of boundary line\n",
    "plt.fill_between(h,v,4.1,color=\"blue\",alpha=0.1)\n",
    "plt.fill_between(h,v,0,color=\"red\",alpha=0.1)\n",
    "\n",
    "plt.plot(h,v,'-')\n",
    "plt.title('epoch= '+str(epoch))\n",
    "plt.show()\n",
    "print('Error = '+ str(E_total(W)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(epoch),error)\n",
    "plt.title('Evolution of error up to '+str(epoch)+ ' iterations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second example : approximate a step function\n",
    "\n",
    "We want to find a neural network realizing a function $F$ such that :\n",
    "* if $x \\in [0;2] \\cup [6;8]$, $F(x)=0$ ;\n",
    "* if $x \\in [3;5]$, $F(x) = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data : training set\n",
    "We consider 10 blue circles on $[0;2]$, 10 blue circles on $[6;8]$ and 10 red squares on $[3;5]$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blue_circles = []\n",
    "target = []\n",
    "X = np.linspace(0,2,10)\n",
    "\n",
    "for x in X:\n",
    "    blue_circles.append((x,0))\n",
    "\n",
    "X = np.linspace(6,8,10)\n",
    "\n",
    "for x in X:\n",
    "    blue_circles.append((x,0))\n",
    "\n",
    "red_squares = []\n",
    "\n",
    "X = np.linspace(3,5,10)\n",
    "\n",
    "for x in X:\n",
    "    red_squares.append((x,1))\n",
    "\n",
    "for x,y in blue_circles:\n",
    "    plt.scatter(x,y,color='blue')\n",
    "for x,y in red_squares:\n",
    "    plt.scatter(x,y,color='red',marker='s')\n",
    "\n",
    "points = blue_circles+red_squares\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose this architecture of neural network :\n",
    "\n",
    "<img src=\"img/propagation_ex2.png\"></img>\n",
    "\n",
    "with the sigmoid activation function. \n",
    "\n",
    "Thus, we are looking for 7 weights $(a_1,a_2,...,a_7)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = [0.0,1.0,0.0,-1.0,1.0,1.0,-1.0]\n",
    "\n",
    "def F(x,W):\n",
    "    layer1 = [sigmoid(W[0]*x+W[1]),sigmoid(W[2]*x+W[3])]\n",
    "    layer2 = sigmoid(W[4]*layer1[0]+W[5]*layer1[1]+W[6])\n",
    "    return layer2\n",
    "\n",
    "def gradF(x,W):\n",
    "    layer1 = [sigmoid(W[0]*x+W[1]),sigmoid(W[2]*x+W[3])]\n",
    "    layer2 = sigmoid(W[4]*layer1[0]+W[5]*layer1[1]+W[6])\n",
    "    dsigma2 = p(layer2)\n",
    "    g5 = layer1[0]*dsigma2\n",
    "    g6 = layer1[1]*dsigma2\n",
    "    g7 = dsigma2\n",
    "    dsigma11 = p(layer1[0])\n",
    "    dsigma12 = p(layer1[1])\n",
    "    g2 = dsigma11/layer1[0]*W[4]*g5\n",
    "    g1 = x * g2\n",
    "    g4 = dsigma12/layer1[1]*W[5]*g6\n",
    "    g3 = x * g4\n",
    "    grad = [g1,g2,g3,g4,g5,g6,g7]\n",
    "    return grad\n",
    "\n",
    "def gradE(W,x,y):\n",
    "    output = F(x,W)\n",
    "    grad = [2*(F(x,W)-y)*g for g in gradF(x,W)]\n",
    "    return grad\n",
    "\n",
    "\n",
    "def E(W,x,t):\n",
    "    return (F(x,W)-t)**2\n",
    "\n",
    "def gradE_total(W):\n",
    "    g = np.array([0,0,0,0,0,0,0])\n",
    "    i=0\n",
    "    for (x,y) in points:\n",
    "        g = np.array(gradE(W,x,y)) + g\n",
    "        i+=1\n",
    "    return g/i\n",
    "\n",
    "def E_total(W):\n",
    "    e = 0.0\n",
    "    i=0\n",
    "    for (x,y) in points:\n",
    "        e = E(W,x,y) + e\n",
    "        i+=1\n",
    "    e = e/(i)\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 5000  #change number of iterations\n",
    "delta = 1\n",
    "error = []\n",
    "\n",
    "for i in range(epoch):\n",
    "    W = W - delta*gradE_total(W)\n",
    "    error.append(E_total(W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(epoch),error)\n",
    "plt.title('Evolution of error up to '+str(epoch)+ ' iterations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for x,y in blue_circles:\n",
    "    plt.scatter(x,y,color='blue')\n",
    "for x,y in red_squares:\n",
    "    plt.scatter(x,y,color='red',marker='s')\n",
    "X = np.linspace(0,8,200)\n",
    "Y = [F(x,W) for x in X]\n",
    "plt.plot(X,Y,color='purple',linewidth=4)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handwritten numbers recognition with TensorFlow \n",
    "\n",
    "https://colab.research.google.com/drive/1bz0-4EU0N1NaLZkJKDyQU0E3W_f_Co8y?usp=sharing\n",
    "\n",
    "# Text recognition\n",
    "https://github.com/exo7math/deepmath-exo7/blob/master/pythontf2/python/tf2_texte_cours.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
